{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a13a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in /Users/lukeroberts/My Drive(lukejrobertsza@gmail.com)/Colab Notebooks/.venv/lib/python3.9/site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in /Users/lukeroberts/My Drive(lukejrobertsza@gmail.com)/Colab Notebooks/.venv/lib/python3.9/site-packages (from surprise) (1.1.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/lukeroberts/My Drive(lukejrobertsza@gmail.com)/Colab Notebooks/.venv/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/lukeroberts/My Drive(lukejrobertsza@gmail.com)/Colab Notebooks/.venv/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/lukeroberts/My Drive(lukejrobertsza@gmail.com)/Colab Notebooks/.venv/lib/python3.9/site-packages (from scikit-surprise->surprise) (1.13.1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 4. Is google colab mandatory for this project?\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Yes, Google Colab is mandatory for this project. In this project a considerable amount of data has to be used and to operate on that, high GPU and RAM will be needed. This is properly incorporated in Google Colab and hence it is mandatory. \"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# df.info()\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"Another way of doing that is \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"6. While working with grid search cv, I am getting a different optimum model than the notebook. How to resolve this?\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mThe one and only reason for this is you are passing a different dataset to the  GridSearchCV code. Trying checking the dataset in terms of number of records, features, and whether correct set of records are  passed or not.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"7.  In the below line of code -\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## faqs\n",
    "\n",
    "# 1. When applying the ranking products function and sorting it based on the corrected ratings, I am getting the below error. \n",
    "\n",
    "\"\"\"KeyError: 'rating_count'\n",
    "How to resolve this error?\n",
    "\n",
    "Below are the possible reasons and the solutions for getting this error:\n",
    "\n",
    "Wrong naming: When you access the column name “rating_count” from the data frame, it should be accessed with the correct name. By the correct name I mean to say that upper case, lower case, and correct spelling of the column have to be taken care of. Along with this, the name of dataframe has also to be correct.\n",
    "The correct function code: The correct code for the function \"ranking_products\" has to be used. If a slight change is made in the function then it is not expected to work properly. Below is the correct code for the “ranking_products” function.\"\"\"\n",
    "\n",
    "def ranking_products(recommendations, final_rating):\n",
    " # Sort the products based on ratings count\n",
    " ranked_products = final_rating.loc[[items[0] for items in recommendations]].sort_values('rating_count', ascending = False)[['rating_count']].reset_index()\n",
    " # Merge with the recommended products to get predicted ratings\n",
    " ranked_products = ranked_products.merge(pd.DataFrame(recommendations, columns = ['prod_id', 'predicted_ratings']), on = 'prod_id', how = 'inner')\n",
    " # Rank the products based on corrected ratings\n",
    " ranked_products['corrected_ratings'] = ranked_products['predicted_ratings'] - 1 / np.sqrt(ranked_products['rating_count'])\n",
    " # Sort the products based on corrected ratings\n",
    " ranked_products = ranked_products.sort_values('corrected_ratings', ascending = False)\n",
    " return ranked_products\n",
    " \n",
    "\n",
    "\"\"\"2.  While recommending the top 5 items with 50 and 100 interactions for the Rank Based Recommendation System, I am getting the duplicate results. How to resolve this?\n",
    "# To resolve this issue, the “top_n_products” function has to be used with the correct code.\n",
    "# Below is the correct code for the function.\"\"\"\n",
    "\n",
    "\n",
    "# Defining a function to get the top n products based on the highest average rating and minimum interactions\n",
    "def top_n_products(final_rating, n, min_interaction):\n",
    " # Finding products with minimum number of interactions\n",
    " recommendations = final_rating[final_rating['rating_count'] > min_interaction]\n",
    " # Sorting values w.r.t average rating\n",
    " recommendations = recommendations.sort_values(by = 'avg_rating', ascending = False)\n",
    " return recommendations.index[:n]\n",
    " \n",
    "\n",
    "\n",
    "# 3. How to install surprise in google colab? \n",
    "# To install surprise in Google Colab, you can use the following line of code - \"\"\"\n",
    "\n",
    "!pip install surprise\n",
    " \n",
    "\n",
    "# 4. Is google colab mandatory for this project?\n",
    "# Yes, Google Colab is mandatory for this project. In this project a considerable amount of data has to be used and to operate on that, high GPU and RAM will be needed. This is properly incorporated in Google Colab and hence it is mandatory. \"\"\"\n",
    "\n",
    " \n",
    "\n",
    "# 5. What is the purpose of checking missing values in the data in this project?\n",
    "# While acquiring data it is common to miss some records. Such records are called as missing values. The basic purpose of checking missing values is to understand the count of Nulls and None in the data. There are few methods of checking missing values in python that are given below - \n",
    "\n",
    "# If df is the name of the dataframe then to check missing values below code can be used.\"\"\"\n",
    "\n",
    "# df.info()\n",
    "\"\"\"Another way of doing that is \"\"\"\n",
    "\n",
    "df.isnull().sum()\n",
    " \n",
    "\n",
    "\"\"\"6. While working with grid search cv, I am getting a different optimum model than the notebook. How to resolve this?\n",
    "The one and only reason for this is you are passing a different dataset to the  GridSearchCV code. Trying checking the dataset in terms of number of records, features, and whether correct set of records are  passed or not.\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "7.  In the below line of code -\"\"\"\n",
    "\n",
    "rows, columns = df_final___\n",
    "\n",
    "\"\"\"How to find the number of rows and columns in a single line of code?\n",
    "For this, the most suitable method in python is the \"shape\" method. df_final.shape will give the requisite output in form of a tuple where the first element is the number of rows and the second element is the number of columns.\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "\"\"\"8. What is meant by a baseline model? Is it same for different algorithms?\n",
    "A baseline model is one with default parameters. For different set of algorithms the model with default parameters will be different and so will be the baseline models.\"\"\"\n",
    "\n",
    " \n",
    "\n",
    "\"\"\"9. In the final dataset, I am having one less number of records. How to resolve this?\n",
    "For this project, the dataset is huge and the number of records used by code depends on how much data was uploaded to GPU/ RAM. Once the dataset is loaded to colab completely, on running the code the whole dataset should appear in the notebook.\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "\"\"\"10. Can the info method used to check the datatypes of the features and also the count of null values in different features?\n",
    "Yes, it can be used because it gives both datatype and the count of null values from a dataframe as its output.\n",
    "\"\"\"\n",
    " \n",
    "\n",
    "\"\"\"11. What does the term \"interactions\" mean in this project?\n",
    "If a user has rated a product it is considered as an interaction between the user and the product. \n",
    "\"\"\"\n",
    " \n",
    "\n",
    "\"\"\"12. What is the meaning of summary statistics of a dataframe in this project (it is asked to find the summary statistics of rating column in the notebook)?\n",
    "The summary of a column in a dataframe includes the count of records, average value, standard deviation, minimum value, the first, second, and the third quartiles, and the maximum value of the column. In python to do so we can use the \"describe\" method on the dataframe/series.\n",
    "\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0786b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
